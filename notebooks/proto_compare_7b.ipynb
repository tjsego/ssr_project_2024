{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example tests compares two models with initial conditions that are different but have the same summary statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import ipywidgets as ipw\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "\n",
    "from stochastic_models import AntimonyModel\n",
    "from stochastic_tests import Test\n",
    "import stochastic_repro as sr\n",
    "\n",
    "sr.start_pool()\n",
    "\n",
    "t_fin = 1.0\n",
    "num_steps = 100\n",
    "test_kwargs = dict(t_fin=t_fin, \n",
    "                   num_steps=num_steps, \n",
    "                   sample_times=[t_fin / num_steps * i for i in range(0, num_steps + 1)], \n",
    "                   trials=[100, 500, 1000, 5000, 10000, 50000], \n",
    "                   )\n",
    "\n",
    "mean = 0.0\n",
    "stdev = 0.25\n",
    "\n",
    "model_string = f\"\"\"\n",
    "-> x ; 2 * pi * k * cos(2 * pi * k * time);\n",
    "-> y ; - 2 * pi * k * sin(2 * pi * k * time);\n",
    "k = 1.0\n",
    "\n",
    "x = 0.0\n",
    "y = 0.0\n",
    "\"\"\"\n",
    "def model(*args, **kwargs):\n",
    "    return AntimonyModel(model_string, ['x', 'y'], *args, **kwargs)\n",
    "\n",
    "test_control = Test(model=model({'x': ('norm', (mean, stdev))}), stochastic=False, **test_kwargs)\n",
    "test_control.trials = [t * 2 for t in test_control.trials]\n",
    "\n",
    "test1 = Test(model=model({'x': ('norm', (mean, stdev))}), stochastic=False, **test_kwargs)\n",
    "test2 = Test(model=model({'x': ('norm', (mean, stdev))}), stochastic=False, **test_kwargs)\n",
    "test3 = Test(model=model({'x': ('laplace', (mean, stdev / np.sqrt(2)))}), stochastic=False, **test_kwargs)\n",
    "test4 = Test(model=model({'x': ('uniform', (mean - stdev * np.sqrt(3), stdev * np.sqrt(12)))}), stochastic=False, **test_kwargs)\n",
    "test5 = Test(model=model({'x': ('logistic', (mean, np.sqrt(3) * stdev / np.pi))}), stochastic=False, **test_kwargs)\n",
    "tests = [test1, test2, test3, test4, test5]\n",
    "labels = ['Normal', 'Normal', 'Laplace', 'Uniform', 'Logistic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in tests:\n",
    "    test.execute_deterministic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, out = ipw.Label(), ipw.Output()\n",
    "display(out)\n",
    "with out:\n",
    "    display(label)\n",
    "\n",
    "for test in tests:\n",
    "    test.execute_stochastic(label)\n",
    "_ = out.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, test in enumerate(tests):\n",
    "    fig = test.plot_results_stochastic()[0]\n",
    "    fig.suptitle(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, test in enumerate(tests):\n",
    "    fig = test.plot_distributions()[0]\n",
    "    fig.suptitle(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_tests = [{t: {n: np.ndarray((num_steps, 2), dtype=float) for n in test1.model.results_names} for t in test_kwargs['trials']} for _ in tests]\n",
    "\n",
    "for t in test_kwargs['trials']:\n",
    "    for n in test1.model.results_names:\n",
    "        for s in range(num_steps):\n",
    "            for i in range(len(tests)):\n",
    "                res = tests[i].sims_s[t].extract_var_index(n, s)\n",
    "                stats_tests[i][t][n][s, :] = np.mean(res), np.std(res)\n",
    "\n",
    "fig, ax = plt.subplots(len(test_kwargs['trials']), len(test1.model.results_names), figsize=(12.0, 2.0 * len(test_kwargs['trials'])), sharex=True, layout='compressed')\n",
    "for i, t in enumerate(test_kwargs['trials']):\n",
    "    for j, n in enumerate(test1.model.results_names):\n",
    "        for si, s in enumerate(stats_tests):\n",
    "            ax[i][j].errorbar(np.asarray(list(range(num_steps)), dtype=float), s[t][n][:, 0], yerr=s[t][n][:, 1], label=labels[si])\n",
    "    ax[i][0].set_ylabel(f'Sample size: {t}')\n",
    "    \n",
    "for j, n in enumerate(test1.model.results_names):\n",
    "    ax[0][j].set_title(n)\n",
    "fig.suptitle('Means +- standard deviations')\n",
    "ax[0][0].legend(labels)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(len(test_kwargs['trials']), len(test1.model.results_names), figsize=(12.0, 2.0 * len(test_kwargs['trials'])), sharex=True, layout='compressed')\n",
    "for i, t in enumerate(test_kwargs['trials']):\n",
    "    for j, n in enumerate(test1.model.results_names):\n",
    "        for sn, s in enumerate(stats_tests):\n",
    "            ax[i][j].plot(np.asarray(list(range(num_steps)), dtype=float), s[t][n][:, 0], label=labels[sn])\n",
    "    ax[i][0].set_ylabel(f'Sample size: {t}')\n",
    "\n",
    "for j, n in enumerate(test1.model.results_names):\n",
    "    ax[0][j].set_title(n)\n",
    "fig.suptitle('Means')\n",
    "ax[0][0].legend(labels)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(len(test_kwargs['trials']), len(test1.model.results_names), figsize=(12.0, 2.0 * len(test_kwargs['trials'])), sharex=True, layout='compressed')\n",
    "for i, t in enumerate(test_kwargs['trials']):\n",
    "    for j, n in enumerate(test1.model.results_names):\n",
    "        for sn, s in enumerate(stats_tests):\n",
    "            ax[i][j].plot(np.asarray(list(range(num_steps)), dtype=float), s[t][n][:, 1], label=labels[sn])\n",
    "    ax[i][0].set_ylabel(f'Sample size: {t}')\n",
    "\n",
    "for j, n in enumerate(test1.model.results_names):\n",
    "    ax[0][j].set_title(n)\n",
    "fig.suptitle('Standard deviations')\n",
    "ax[0][0].legend(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the converged ECF of the two models, we should find that they are always quantifiably different since their models have different initial conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_control.execute_stochastic()\n",
    "test_control.find_ecfs()\n",
    "test_control.test_sampling(err_thresh=1E-3)\n",
    "for test in tests:\n",
    "    test.find_ecfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecf_diff_all = [{t: [] for t in test_kwargs['trials']} for _ in range(len(tests) - 1)]\n",
    "\n",
    "trial_oi = test_kwargs['trials'][-1]\n",
    "idx_oi = 0\n",
    "err_max_val = 0\n",
    "\n",
    "for t in test_kwargs['trials']:\n",
    "    for i in range(num_steps):\n",
    "        res_1s_n = [{} for _ in range(len(tests) - 1)]\n",
    "        for n in test1.model.results_names:\n",
    "            eval_t = sr.get_eval_info_times(test1.ecf_eval_info[t][i][n], False)\n",
    "\n",
    "            ecf_1 = sr.ecf(test1.sims_s[t].results[n].T[i, :], eval_t)\n",
    "\n",
    "            for j in range(len(tests) - 1):\n",
    "                ecf_j = sr.ecf(tests[j+1].sims_s[t].results[n].T[i, :], eval_t)\n",
    "                res_1s_n[j][n] = sr.ecf_compare(ecf_1[:, 0], ecf_1[:, 1], ecf_j[:, 0], ecf_j[:, 1])\n",
    "\n",
    "        for j in range(len(tests) - 1):\n",
    "            ecf_diff_all[j][t].append(res_1s_n[j])\n",
    "\n",
    "        if t == trial_oi:\n",
    "            err_max_val_i = max([max(res.values()) for res in res_1s_n])\n",
    "            if err_max_val_i > err_max_val:\n",
    "                idx_oi = i\n",
    "                err_max_val = err_max_val_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.errorbar([int(t/2) for t in test_control.trials], \n",
    "            [np.average(test_control.ks_stats_sampling[t]) for t in test_control.trials],\n",
    "            yerr=[np.std(test_control.ks_stats_sampling[t]) * 3 for t in test_control.trials],\n",
    "            marker='o', label='Repro. test')\n",
    "for j in range(len(tests[1:])):\n",
    "    ax.plot(test1.trials, [max([max(el.values()) for el in ecf_diff_all[j][trial]]) for trial in test1.trials], marker='o', label=labels[j+1])\n",
    "ax.set_xlabel('Sample size')\n",
    "ax.set_ylabel('EFECT error')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6.0, 3.0), layout='compressed')\n",
    "for j, n in enumerate(test1.model.results_names):\n",
    "    for i, t in enumerate(tests):\n",
    "        ax[j].hist(t.sims_s[trial_oi].results[n].T[idx_oi, :], label=labels[i], alpha=0.5)\n",
    "    ax[j].set_title(n)\n",
    "ax[0].legend(labels)\n",
    "\n",
    "for k, t in enumerate(tests[1:]):\n",
    "    fig, ax = plt.subplots(3, 2, figsize=(6.0, 6.0), sharex='col', layout='compressed')\n",
    "    for i, n in enumerate(test1.model.results_names):\n",
    "        eval_t = sr.get_eval_info_times(test1.ecf_eval_info[trial_oi][idx_oi][n], False)\n",
    "        ecf_1 = sr.ecf(test1.sims_s[trial_oi].results[n].T[idx_oi, :], eval_t)\n",
    "        ecf_2 = sr.ecf(t.sims_s[trial_oi].results[n].T[idx_oi, :], eval_t)\n",
    "\n",
    "        for j in range(2):\n",
    "            ax[j][i].plot(eval_t, ecf_1[:, j])\n",
    "            ax[j][i].plot(eval_t, ecf_2[:, j])\n",
    "\n",
    "        d12 = np.sqrt(np.square(ecf_1[:, 0] - ecf_2[:, 0]) + np.square(ecf_1[:, 1] - ecf_2[:, 1]))\n",
    "        ax[2][i].plot(eval_t, d12)\n",
    "    fig.suptitle(f'Normal vs. {labels[k+1]}')\n",
    "    ax[0][0].set_ylabel('Real')\n",
    "    ax[1][0].set_ylabel('Imaginary')\n",
    "    ax[2][0].set_ylabel('EFECT error')\n",
    "for i, n in enumerate(test1.model.results_names):\n",
    "    ax[0][i].set_title(n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stoch_repro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
