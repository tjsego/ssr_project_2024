{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example tests whether an approach that uses empirical characteristic functions (ECFs) can detect differences in models with different parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import ipywidgets as ipw\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from stochastic_models import model_bistable2\n",
    "from stochastic_tests import Test\n",
    "import stochastic_repro as sr\n",
    "\n",
    "t_fin = 10.0\n",
    "num_steps = 1000\n",
    "test_kwargs = dict(t_fin=t_fin, \n",
    "                   num_steps=num_steps, \n",
    "                   sample_times=[t_fin / num_steps * i for i in range(0, num_steps + 1)], \n",
    "                   trials=[100, 500, 1000, 5000, 10000])\n",
    "\n",
    "a1_mean = 0.9\n",
    "a2_mean = 1.1\n",
    "\n",
    "test1 = Test(model=model_bistable2({'a': ('norm', (a1_mean, 0.2))}), stochastic=False, **test_kwargs)\n",
    "test2 = Test(model=model_bistable2({'a': ('norm', (a2_mean, 0.2))}), stochastic=False, **test_kwargs)\n",
    "tests = [test1, test2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in tests:\n",
    "    test.execute_deterministic()\n",
    "    _ = test.plot_results_deterministic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, out = ipw.Label(), ipw.Output()\n",
    "display(out)\n",
    "with out:\n",
    "    display(label)\n",
    "\n",
    "for test in tests:\n",
    "    test.execute_stochastic(label)\n",
    "_ = out.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in tests:\n",
    "    _ = test.plot_results_stochastic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in tests:\n",
    "    _ = test.plot_distributions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: ~159 minutes (M1 max)\n",
    "for test in [test1, test2]:\n",
    "    test.find_ecfs()\n",
    "    test.measure_ecf_diffs()\n",
    "    test.plot_ecf(test.max_ks_stat_time(test.trials[-1]))\n",
    "    test.plot_ecf_diffs()\n",
    "    test.test_sampling(err_thresh=1E-3)\n",
    "    test.generate_ecf_sampling_fits()\n",
    "    _, ax = test.plot_ecf_sampling_fits(test.plot_ecf_sampling())\n",
    "    _ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the converged ECF of the two models, we should find that they are always quantifiably different since their models have different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecf_diff = sr.measure_ecf_diff_sets(test1.ecf, test2.ecf)\n",
    "\n",
    "fig, ax = plt.subplots(1, len(test1.model.results_names), sharey=False, figsize=(12.0, 2.0), layout='compressed')\n",
    "for i, name in enumerate(test1.model.results_names):\n",
    "    ax[i].scatter(test1.trials, [max([el[name] for el in ecf_diff[trial]]) for trial in test1.trials])\n",
    "    ax[i].set_xlabel('Sample size')\n",
    "    ax[i].set_xscale('log')\n",
    "    ax[i].set_title(name)\n",
    "ax[0].set_ylabel('EFECT error')\n",
    "_ = fig.suptitle('Test for reproducibility per variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for how well the method can detect differences in model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_fact = [0.5, 0.75, 0.9, 0.95, 0.99, 1.0, 1.01, 1.05, 1.1, 1.5, 2.0]\n",
    "a_comp = list()\n",
    "test_test_kwargs = test_kwargs.copy()\n",
    "test_test_kwargs['trials'] = [int(t / 2) for t in test_kwargs['trials']]\n",
    "\n",
    "label, out = ipw.Label(), ipw.Output()\n",
    "display(out)\n",
    "with out:\n",
    "    display(label)\n",
    "\n",
    "# Make half-size tests\n",
    "test1_sims_hs = {}\n",
    "ecf_eval_info = {}\n",
    "for trials in test1.trials:\n",
    "    ht = int(trials / 2)\n",
    "    simset = sr.SimSet(model=test1.model, num_trials=ht, stochastic=False, num_steps=test_kwargs['num_steps'], t_fin=test_kwargs['t_fin'])\n",
    "    simset.results = {k: v[:ht, :] for k, v in test1.sims_s[trials].results.items()}\n",
    "    simset.results_time = test1.sims_s[trials].results_time\n",
    "    test1_sims_hs[ht] = simset\n",
    "    ecf_eval_info[ht] = test1.ecf_eval_info[trials]\n",
    "ecf1 = sr.generate_ecfs(test1_sims_hs, test1.model.results_names, test_test_kwargs['trials'], ecf_eval_info)\n",
    "\n",
    "for af in a_fact:\n",
    "    label.value = f'Factor: {af}...'\n",
    "\n",
    "    atest_mean = a1_mean * af\n",
    "    test = Test(model=model_bistable2({'a': ('norm', (atest_mean, 0.2))}), stochastic=False, **test_test_kwargs)\n",
    "    test.execute_stochastic()\n",
    "    ecf2 = sr.generate_ecfs(test.sims_s, test1.model.results_names, test_test_kwargs['trials'], ecf_eval_info)\n",
    "    a_comp.append(sr.measure_ecf_diff_sets(ecf1, ecf2))\n",
    "\n",
    "out.clear_output()\n",
    "\n",
    "fig, ax = plt.subplots(len(test1.trials), len(test1.model.results_names), sharey='row', figsize=(12.0, 2.0 * len(test1.trials)), layout='compressed')\n",
    "for i, trial in enumerate(test1.trials):\n",
    "    for j, name in enumerate(test1.model.results_names):\n",
    "        ax[i][j].scatter(a_fact, [max([el[name] for el in diff[int(trial / 2)]]) for diff in a_comp])\n",
    "        \n",
    "        avg = np.average(test1.ks_stats_sampling[trial])\n",
    "        err = np.std(test1.ks_stats_sampling[trial]) * 3\n",
    "        ax[i][j].fill_between(a_fact, avg - err, avg + err, color='lightgray', alpha=0.5)\n",
    "        ax[i][j].set_yscale('log')\n",
    "\n",
    "for i, name in enumerate(test1.model.results_names):\n",
    "    ax[0][i].set_title(name)\n",
    "    ax[-1][i].set_xlabel('Parameter ratio')\n",
    "for i, trial in enumerate(test1.trials):\n",
    "    ax[i][0].set_ylabel(f'Sample size: {trial}')\n",
    "_ = fig.suptitle('EFECT error per parametric difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_entries = ['Average', 'St. dev.']\n",
    "df_summary = pd.DataFrame(index=test_kwargs['trials'], columns=summary_entries)\n",
    "eval_entries = ['Sample size', 'Scaling factor', 'EFECT error', 'Standardized error', 'Acceptance p-value']\n",
    "data_eval = []\n",
    "\n",
    "for sample_size in test_test_kwargs['trials']:\n",
    "    avg = np.average(test1.ks_stats_sampling[sample_size * 2])\n",
    "    std = np.std(test1.ks_stats_sampling[sample_size * 2])\n",
    "    df_summary.loc[sample_size * 2, 'Average'] = avg\n",
    "    df_summary.loc[sample_size * 2, 'St. dev.'] = std\n",
    "\n",
    "    err = [max([max(d.values()) for d in diff[sample_size]]) for diff in a_comp]\n",
    "    q2 = (sample_size + 1) / sample_size * np.var(test1.ks_stats_sampling[sample_size * 2], ddof=1)\n",
    "    lam2 = [(e - avg) * (e - avg) / q2 for e in err]\n",
    "    pr = [np.floor((sample_size + 1) / sample_size * ((sample_size - 1) / l2 + 1)) / (sample_size + 1) for l2 in lam2]\n",
    "    for i in range(len(err)):\n",
    "        if pr[i] > 1.0 or err[i] < avg:\n",
    "            pr[i] = 1.0\n",
    "\n",
    "    for i, bf in enumerate(a_fact):\n",
    "        data_eval.append((sample_size * 2, bf, err[i], (err[i] - avg) / std, min(1, pr[i])))\n",
    "\n",
    "display(df_summary)\n",
    "mi_eval = pd.MultiIndex.from_frame(pd.DataFrame(data_eval, columns=eval_entries))\n",
    "df_eval = mi_eval.to_frame()\n",
    "display(df_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stoch_repro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
