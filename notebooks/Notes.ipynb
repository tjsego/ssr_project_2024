{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8cc5066",
   "metadata": {},
   "source": [
    "Stochastic Simulation Reproducibility\n",
    "======================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa369cd7",
   "metadata": {},
   "source": [
    "Theoretical Notes\n",
    "------------------\n",
    "\n",
    "By definition, a stochastic process is a sequence of random variables on the real line. Therefore, sampling a stochastic process at a given time is the same as sampling a random variable.\n",
    "\n",
    "The distribution of a stochastic process is not unique. Therefore, we cannot expect that a given distribution uniquely identifies a stochastic process. However, the characteristic function (CF) of a stochastic process perfectly describes the distribution of the stochastic process at each sampling time. It is well known that the \"empirical characteristic function\" (ECF) constructed from realizations of a stochastic process tends towards the CF of the stochastic process as the number of realizations increases. Therefore, we can compare realizations of a stochastic process by comparing sufficiently accurate ECFs of those realizations. Plainly put, two sets of realizations of a stochastic model can be compared by comparing sufficiently accurate ECFs of the sets. \n",
    "\n",
    "Testing for reproducible results: when increasing the number of realizations of a stochastic process does not significantly change the ECFs constructed from those realizations, then the ECFs are a reliable (i.e., sufficiently accurate) approximation of the CF of the stochastic process at each measured time.\n",
    "\n",
    "Testing for reproduced results: if $N$ realizations of a stochastic process produce an ECF that is a reliable approximation of the CF of the stochastic process at each measured time, then another set of $N$ realizations of the same stochastic process will produce the same ECFs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf0a73f",
   "metadata": {},
   "source": [
    "On Using Empirical Characteristic Functions (needs updated)\n",
    "--------------------------------------------\n",
    "\n",
    "An ECF is a function of an arbitrary variable. Hence, to demonstrate reproducible or reproduced results one must demonstrate sufficient similarity of sufficiently representative ECFs at each sample time. \n",
    "\n",
    "The first problem to address is how to unambiguously show convergence of the ECF by the modeler. The Kolmogrov-Smirnov (K-S) statistic suffices to measure similarity of two ECFs, however one needs to define the domain over which the ECF is evaluated at each sample time and the interval of evaluation. \n",
    "\n",
    "For any sample size, a modeler can choose a sufficiently small domain to produce an arbitrarily small K-S statistic. Likewise a modeler can choose a sufficiently large domain to produce an ECF that is everywhere near zero (with a sufficiently coarse interval of evaluation). Both cases are mitigated if the ECF is evaluated over a domain that is scaled such that the unique features of the ECF are sufficiently captured, which should be somehow related to the variance of the sample. \n",
    "\n",
    "Todo: try recasting the ECF using a standardized variable of the sampled variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aad412b",
   "metadata": {},
   "source": [
    "Workflows (needs updated)\n",
    "----------\n",
    "\n",
    "Proposed criteria for reproducibility of a stochastic ODE model\n",
    "\n",
    "1. Sampling distributions of all stochastic variables should converge at all sample times with increasing sample size. \n",
    "2. Results distributions can be reproduced for all stochastic variables at all sample time points for the same sample size. \n",
    "\n",
    "Proposed workflow (modeler)\n",
    "\n",
    "1. Run sample size $2N$ of a stochastic model while sampling the same stochastic variables and sample times for all sample replicates. \n",
    "2. Evenly divide the sample into two subsamples, each of size $N$. \n",
    "3. At each sample time, construct an ECF for each variable of both subsamples at each sample time. \n",
    "4. If the difference of the two ECFs for each variable at all sample times is below a threshold, then consider sample size $M$ sufficient to reproduce the simulation results, where $M \\geq N$. Otherwise, increase $N$ and return to 1. \n",
    "\n",
    "Proposed workflow (curator)\n",
    "\n",
    "1. Implement the reported model. \n",
    "2. Generate a sample of size $M$.\n",
    "3. At each reported sample time, construct an ECF for each variable. \n",
    "4. If the difference of the generated and reported ECFs for each variable at all sample times is below a threshold, then consider the curated model reproduced. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b65160",
   "metadata": {},
   "source": [
    "This example uses a simple \"SIR\" model of viral infection. The model is a system of ordinary differential equations that describes the infection of a population of susceptible cells $S$ by a virus $V$. Infection turns susceptible cells into infected cells $I$, which release virus and later become recovered cells $R$. Recovered cells do not release virus and are not susceptible to infection. \n",
    "\n",
    "$$\n",
    "\\frac{dS}{dt} = - \\beta S V \\\\\n",
    "\\frac{dI}{dt} = \\beta S V - \\delta I \\\\\n",
    "\\frac{dR}{dt} = \\delta I \\\\\n",
    "\\frac{dV}{dt} = p I - k V\n",
    "$$\n",
    "\n",
    "This example introduces variations through random sampling of the infectivity parameter $\\beta$ using a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0531a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "from IPython.display import display\n",
    "\n",
    "from stochastic_repro import start_pool\n",
    "from stochastic_models import model_sir\n",
    "from stochastic_tests import Test\n",
    "\n",
    "t_fin = 10.0\n",
    "num_steps = 100\n",
    "\n",
    "test = Test(model=model_sir({'beta': ('norm', (2.0E-6, 0.2E-6))}),\n",
    "            t_fin=t_fin, \n",
    "            num_steps=num_steps,\n",
    "            sample_times=[t_fin / num_steps * i for i in range(0, num_steps + 1)],\n",
    "            trials=[10, 100, 1000, 10000],\n",
    "            stochastic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc5d405",
   "metadata": {},
   "source": [
    "The SIR model is characterized by transition of susceptible cells to infected cells and then to refractory cells, and a peak in virus that correlates with the peak in infected cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47f0e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.execute_deterministic()\n",
    "_ = test.plot_results_deterministic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d076c2c7",
   "metadata": {},
   "source": [
    "As we increase the number of replicates, we should find that statistical measures of the replicates in time converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19158f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label, out = ipw.Label(), ipw.Output()\n",
    "display(out)\n",
    "with out:\n",
    "    display(label)\n",
    "\n",
    "start_pool()\n",
    "test.execute_stochastic(label)\n",
    "_ = out.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce6c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test.plot_results_stochastic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c78fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test.plot_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86ddc2a",
   "metadata": {},
   "source": [
    "If we bin replicate results at each sample time, we should find that the probability distribution function of a variable at each time point converges with increasing number of replicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcbc688",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test.plot_distributions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4a1c09",
   "metadata": {},
   "source": [
    "A measure of convergence could be to calculate the average absolute relative difference in probability of each bin at each sampled time, where the change is taken with respect to the minimum of two compared probabilities. The basic procedure to calculate this metric for a set of replicates is as follows:\n",
    "\n",
    "1. Evenly divide the set of replicates into two subsets. \n",
    "2. For each variable and sampled time, calculate an approximate probability distribution function for each subset using a typical histogram algorithm. Note that binning of each subset must use the same intervals. \n",
    "3. For each interval, calculate the maximum absolute relative difference between the approximate probability distribution function of each subset. The maximum is determined by calculating the difference relative to the lesser of the two probabilities of the interval. \n",
    "4. Take the mean of 3. over all intervals and sampled times to quantify the change for a variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c6d80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar, out = ipw.IntProgress(), ipw.Output()\n",
    "# progress_bar.max = (len(test.trials) - 1) * len(test.model.results_names)\n",
    "# display(out)\n",
    "\n",
    "# with out:\n",
    "#     label = ipw.Label(value='Calculating differences...')\n",
    "#     vbox = ipw.VBox([label, progress_bar])\n",
    "#     display(vbox)\n",
    "\n",
    "# test.measure_dist_diff_basic(progress_bar=progress_bar)\n",
    "# out.clear_output()\n",
    "\n",
    "# _ = test.plot_dist_diff(test.acc_diff_basic[0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d417f8ca",
   "metadata": {},
   "source": [
    "However, this metric does not seem to capture the similarity between two subsets of a set of replicates. For example, the final number of replicates seems to be sufficiently similar by inspection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36232fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = test.plot_distributions_compare(test.trials[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0b663f",
   "metadata": {},
   "source": [
    "Applying a filter produces a metric that shows convergence. In this case, the probabilities of a variable at a sample time are not compared if the minimum of the two probabilities is below a threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86564a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar, out = ipw.IntProgress(), ipw.Output()\n",
    "# progress_bar.max = (len(test.trials) - 1) * len(test.model.results_names)\n",
    "# display(out)\n",
    "\n",
    "# with out:\n",
    "#     label = ipw.Label(value='Calculating differences...')\n",
    "#     vbox = ipw.VBox([label, progress_bar])\n",
    "#     display(vbox)\n",
    "\n",
    "# test.measure_dist_diff_basic(filter=0.05, progress_bar=progress_bar)\n",
    "# out.clear_output()\n",
    "\n",
    "# _ = test.plot_dist_diff(test.acc_diff_basic[0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef02a4",
   "metadata": {},
   "source": [
    "Kullback-Leibler Divergence might be a viable comparator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar, out = ipw.IntProgress(), ipw.Output()\n",
    "# progress_bar.max = len(test.trials) * len(test.model.results_names)\n",
    "# display(out)\n",
    "\n",
    "# with out:\n",
    "#     label = ipw.Label(value='Calculating differences...')\n",
    "#     vbox = ipw.VBox([label, progress_bar])\n",
    "#     display(vbox)\n",
    "\n",
    "# test.measure_dist_div_kldiv(progress_bar=progress_bar)\n",
    "# out.clear_output()\n",
    "\n",
    "# _ = test.plot_dist_diff(test.acc_diff_kl_div)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2890ed67",
   "metadata": {},
   "source": [
    "Testing for correlation between results sets might be a viable comparator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce4e5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = ipw.Output()\n",
    "# display(out)\n",
    "\n",
    "# with out:\n",
    "#     print('Analyzing correlations...')\n",
    "# test.measure_correlation()\n",
    "# out.clear_output()\n",
    "\n",
    "# _ = test.plot_correlations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f5ed30",
   "metadata": {},
   "source": [
    "Testing various standard tests here, looking for a way to assign a p-value to showing reproducibility. None of them seem to work well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1951c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from matplotlib import pyplot as plt\n",
    "# from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d6d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing the relevance of the Kolmogorov–Smirnov test\n",
    "\n",
    "\n",
    "# def ks_test(res_1: np.ndarray, res_2: np.ndarray):\n",
    "#     return abs(stats.ks_2samp(res_1, res_2).statistic)\n",
    "\n",
    "\n",
    "# def ks_test_pval(res_1: np.ndarray, res_2: np.ndarray):\n",
    "#     return stats.ks_2samp(res_1, res_2).pvalue\n",
    "\n",
    "\n",
    "# conv_measure_ks_test = {name: {trial: list() for trial in test.trials} for name in test.model.results_names}\n",
    "# conv_measure_ks_pval = {name: {trial: list() for trial in test.trials} for name in test.model.results_names}\n",
    "\n",
    "# for name in test.model.results_names:\n",
    "#     for trial in test.trials:\n",
    "#         n = int(trial / 2)\n",
    "#         for sample_time in test.sample_times:\n",
    "#             res = test.sims_s[trial].extract_var_time(name, sample_time)\n",
    "#             res_1, res_2 = res[:n], res[n:]\n",
    "#             conv_measure_ks_test[name][trial].append(ks_test(res_1, res_2))\n",
    "#             conv_measure_ks_pval[name][trial].append(ks_test_pval(res_1, res_2))\n",
    "\n",
    "# fig, ax = plt.subplots(len(test.trials), len(test.model.results_names), sharey=False, figsize=(12.0, 2.0 * len(test.trials)), layout='compressed')\n",
    "\n",
    "# for i, trial in enumerate(test.trials):\n",
    "#     for j, name in enumerate(test.model.results_names):\n",
    "#         res = conv_measure_ks_test[name][trial]\n",
    "#         ax[i][j].plot(test.sample_times, res)\n",
    "#     ax[i][0].set_ylabel(f'Trials: {trial}')\n",
    "# for j, name in enumerate(test.model.results_names):\n",
    "#     ax[0][j].set_title(name)\n",
    "# fig.suptitle('Kolmogorov–Smirnov test (values)')\n",
    "\n",
    "# fig, ax = plt.subplots(len(test.trials), len(test.model.results_names), sharey=True, figsize=(12.0, 2.0 * len(test.trials)), layout='compressed')\n",
    "\n",
    "# for i, trial in enumerate(test.trials):\n",
    "#     for j, name in enumerate(test.model.results_names):\n",
    "#         res = conv_measure_ks_pval[name][trial]\n",
    "#         ax[i][j].plot(test.sample_times, res)\n",
    "#     ax[i][0].set_ylabel(f'Trials: {trial}')\n",
    "#     ax[i][0].set_ylim(-0.05, 1.05)\n",
    "# for j, name in enumerate(test.model.results_names):\n",
    "#     ax[0][j].set_title(name)\n",
    "# _ = fig.suptitle('Kolmogorov–Smirnov test (p-values)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9dffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing the relevance of the Student t-test\n",
    "\n",
    "\n",
    "# def t_test(res_1: np.ndarray, res_2: np.ndarray):\n",
    "#     return abs(stats.ttest_ind(res_1, res_2).statistic)\n",
    "\n",
    "\n",
    "# def t_test_pval(res_1: np.ndarray, res_2: np.ndarray):\n",
    "#     return stats.ttest_ind(res_1, res_2).pvalue\n",
    "\n",
    "\n",
    "# conv_measure_t_test = {name: {trial: list() for trial in test.trials} for name in test.model.results_names}\n",
    "# conv_measure_t_pval = {name: {trial: list() for trial in test.trials} for name in test.model.results_names}\n",
    "\n",
    "# for name in test.model.results_names:\n",
    "#     for trial in test.trials:\n",
    "#         n = int(trial / 2)\n",
    "#         for sample_time in test.sample_times:\n",
    "#             res = test.sims_s[trial].extract_var_time(name, sample_time)\n",
    "#             res_1, res_2 = res[:n], res[n:]\n",
    "#             conv_measure_t_test[name][trial].append(t_test(res_1, res_2))\n",
    "#             conv_measure_t_pval[name][trial].append(t_test_pval(res_1, res_2))\n",
    "\n",
    "# fig, ax = plt.subplots(len(test.trials), len(test.model.results_names), sharey=False, figsize=(12.0, 2.0 * len(test.trials)), layout='compressed')\n",
    "\n",
    "# for i, trial in enumerate(test.trials):\n",
    "#     for j, name in enumerate(test.model.results_names):\n",
    "#         ax[i][j].plot(test.sample_times, conv_measure_t_test[name][trial])\n",
    "#         ax[i][j].set_yscale('log')\n",
    "        \n",
    "#     ax[i][0].set_ylabel(f'Trials: {trial}')\n",
    "# for j, name in enumerate(test.model.results_names):\n",
    "#     ax[0][j].set_title(name)\n",
    "# fig.suptitle('T-test (values)')\n",
    "\n",
    "# fig, ax = plt.subplots(len(test.trials), len(test.model.results_names), sharey=True, figsize=(12.0, 2.0 * len(test.trials)), layout='compressed')\n",
    "\n",
    "# for i, trial in enumerate(test.trials):\n",
    "#     for j, name in enumerate(test.model.results_names):\n",
    "#         ax[i][j].plot(test.sample_times, conv_measure_t_pval[name][trial])\n",
    "#     ax[i][0].set_ylabel(f'Trials: {trial}')\n",
    "#     ax[i][0].set_ylim(-0.05, 1.05)\n",
    "# for j, name in enumerate(test.model.results_names):\n",
    "#     ax[0][j].set_title(name)\n",
    "# _ = fig.suptitle('T-test (p-values)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8851aa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing the relevance of the Cramér-von Mises test\n",
    "\n",
    "\n",
    "# def cvm_test(res_1: np.ndarray, res_2: np.ndarray):\n",
    "#     return abs(stats.cramervonmises_2samp(res_1, res_2).statistic)\n",
    "\n",
    "\n",
    "# def cvm_test_pval(res_1: np.ndarray, res_2: np.ndarray):\n",
    "#     return stats.cramervonmises_2samp(res_1, res_2).pvalue\n",
    "\n",
    "\n",
    "# conv_measure_t_test = {name: {trial: list() for trial in test.trials} for name in test.model.results_names}\n",
    "# conv_measure_t_pval = {name: {trial: list() for trial in test.trials} for name in test.model.results_names}\n",
    "\n",
    "# for name in test.model.results_names:\n",
    "#     for trial in test.trials:\n",
    "#         n = int(trial / 2)\n",
    "#         for sample_time in test.sample_times:\n",
    "#             res = test.sims_s[trial].extract_var_time(name, sample_time)\n",
    "#             res_1, res_2 = res[:n], res[n:]\n",
    "#             conv_measure_t_test[name][trial].append(cvm_test(res_1, res_2))\n",
    "#             conv_measure_t_pval[name][trial].append(cvm_test_pval(res_1, res_2))\n",
    "\n",
    "# fig, ax = plt.subplots(len(test.trials), len(test.model.results_names), sharey=False, figsize=(12.0, 2.0 * len(test.trials)), layout='compressed')\n",
    "\n",
    "# for i, trial in enumerate(test.trials):\n",
    "#     for j, name in enumerate(test.model.results_names):\n",
    "#         ax[i][j].plot(test.sample_times, conv_measure_t_test[name][trial])\n",
    "#         ax[i][j].set_yscale('log')\n",
    "        \n",
    "#     ax[i][0].set_ylabel(f'Trials: {trial}')\n",
    "# for j, name in enumerate(test.model.results_names):\n",
    "#     ax[0][j].set_title(name)\n",
    "# fig.suptitle('Cramér-von Mises test (values)')\n",
    "\n",
    "# fig, ax = plt.subplots(len(test.trials), len(test.model.results_names), sharey=True, figsize=(12.0, 2.0 * len(test.trials)), layout='compressed')\n",
    "\n",
    "# for i, trial in enumerate(test.trials):\n",
    "#     for j, name in enumerate(test.model.results_names):\n",
    "#         ax[i][j].plot(test.sample_times, conv_measure_t_pval[name][trial])\n",
    "#     ax[i][0].set_ylabel(f'Trials: {trial}')\n",
    "#     ax[i][0].set_ylim(-0.05, 1.05)\n",
    "# for j, name in enumerate(test.model.results_names):\n",
    "#     ax[0][j].set_title(name)\n",
    "# _ = fig.suptitle('Cramér-von Mises test (p-values)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b450e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing the relevance of the Kruskal-Wallis H-test\n",
    "\n",
    "\n",
    "# def kwh_test(res_1: np.ndarray, res_2: np.ndarray):\n",
    "#     return abs(stats.kruskal(res_1, res_2).statistic)\n",
    "\n",
    "\n",
    "# def kwh_test_pval(res_1: np.ndarray, res_2: np.ndarray):\n",
    "#     return stats.kruskal(res_1, res_2).pvalue\n",
    "\n",
    "\n",
    "# conv_measure_t_test = {name: {trial: list() for trial in test.trials} for name in test.model.results_names}\n",
    "# conv_measure_t_pval = {name: {trial: list() for trial in test.trials} for name in test.model.results_names}\n",
    "\n",
    "# for name in test.model.results_names:\n",
    "#     for trial in test.trials:\n",
    "#         n = int(trial / 2)\n",
    "#         for sample_time in test.sample_times[1:]:\n",
    "#             res = test.sims_s[trial].extract_var_time(name, sample_time)\n",
    "#             res_1, res_2 = res[:n], res[n:]\n",
    "#             conv_measure_t_test[name][trial].append(kwh_test(res_1, res_2))\n",
    "#             conv_measure_t_pval[name][trial].append(kwh_test_pval(res_1, res_2))\n",
    "\n",
    "# fig, ax = plt.subplots(len(test.trials), len(test.model.results_names), sharey=False, figsize=(12.0, 2.0 * len(test.trials)), layout='compressed')\n",
    "\n",
    "# for i, trial in enumerate(test.trials):\n",
    "#     for j, name in enumerate(test.model.results_names):\n",
    "#         ax[i][j].plot(test.sample_times[1:], conv_measure_t_test[name][trial])\n",
    "#         ax[i][j].set_yscale('log')\n",
    "        \n",
    "#     ax[i][0].set_ylabel(f'Trials: {trial}')\n",
    "# for j, name in enumerate(test.model.results_names):\n",
    "#     ax[0][j].set_title(name)\n",
    "# fig.suptitle('Kruskal-Wallis H-test (values)')\n",
    "\n",
    "# fig, ax = plt.subplots(len(test.trials), len(test.model.results_names), sharey=True, figsize=(12.0, 2.0 * len(test.trials)), layout='compressed')\n",
    "\n",
    "# for i, trial in enumerate(test.trials):\n",
    "#     for j, name in enumerate(test.model.results_names):\n",
    "#         ax[i][j].plot(test.sample_times[1:], conv_measure_t_pval[name][trial])\n",
    "#     ax[i][0].set_ylabel(f'Trials: {trial}')\n",
    "#     ax[i][0].set_ylim(-0.05, 1.05)\n",
    "# for j, name in enumerate(test.model.results_names):\n",
    "#     ax[0][j].set_title(name)\n",
    "# _ = fig.suptitle('Kruskal-Wallis H-test (p-values)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9edecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing the relevance of the Anderson-Darling test\n",
    "\n",
    "\n",
    "# def ad_test(res_1: np.ndarray, res_2: np.ndarray):\n",
    "#     return abs(stats.anderson_ksamp([res_1, res_2]).statistic)\n",
    "\n",
    "\n",
    "# def ad_test_pval(res_1: np.ndarray, res_2: np.ndarray):\n",
    "#     return stats.anderson_ksamp([res_1, res_2]).pvalue\n",
    "\n",
    "\n",
    "# conv_measure_t_test = {name: {trial: list() for trial in test.trials} for name in test.model.results_names}\n",
    "# conv_measure_t_pval = {name: {trial: list() for trial in test.trials} for name in test.model.results_names}\n",
    "\n",
    "# for name in test.model.results_names:\n",
    "#     for trial in test.trials:\n",
    "#         n = int(trial / 2)\n",
    "#         for sample_time in test.sample_times[1:]:\n",
    "#             res = test.sims_s[trial].extract_var_time(name, sample_time)\n",
    "#             res_1, res_2 = res[:n], res[n:]\n",
    "#             conv_measure_t_test[name][trial].append(ad_test(res_1, res_2))\n",
    "#             conv_measure_t_pval[name][trial].append(ad_test_pval(res_1, res_2))\n",
    "\n",
    "# fig, ax = plt.subplots(len(test.trials), len(test.model.results_names), sharey=False, figsize=(12.0, 2.0 * len(test.trials)), layout='compressed')\n",
    "\n",
    "# for i, trial in enumerate(test.trials):\n",
    "#     for j, name in enumerate(test.model.results_names):\n",
    "#         ax[i][j].plot(test.sample_times[1:], conv_measure_t_test[name][trial])\n",
    "#         ax[i][j].set_yscale('log')\n",
    "        \n",
    "#     ax[i][0].set_ylabel(f'Trials: {trial}')\n",
    "# for j, name in enumerate(test.model.results_names):\n",
    "#     ax[0][j].set_title(name)\n",
    "# fig.suptitle('Anderson-Darling test (values)')\n",
    "\n",
    "# fig, ax = plt.subplots(len(test.trials), len(test.model.results_names), sharey=True, figsize=(12.0, 2.0 * len(test.trials)), layout='compressed')\n",
    "\n",
    "# for i, trial in enumerate(test.trials):\n",
    "#     for j, name in enumerate(test.model.results_names):\n",
    "#         ax[i][j].plot(test.sample_times[1:], conv_measure_t_pval[name][trial])\n",
    "#     ax[i][0].set_ylabel(f'Trials: {trial}')\n",
    "#     ax[i][0].set_ylim(-0.05, 1.05)\n",
    "# for j, name in enumerate(test.model.results_names):\n",
    "#     ax[0][j].set_title(name)\n",
    "# _ = fig.suptitle('Anderson-Darling test (p-values)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2622414a",
   "metadata": {},
   "source": [
    "The first plot shows ECFs for each variable and number of trials. Top row shows the real components. Bottom row shows the imaginary components. \n",
    "\n",
    "The second plot shows a well-known metrics for comparing distributions, applied to the modeler workflow described at the beginning of this notebook. \n",
    "\n",
    "The third and fourth plots show the real and imaginary components of the ECF for evenly divided samples of each set of trials, as in the modeler workflow described at the beginning of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e080ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.find_ecfs()\n",
    "test.measure_ecf_diffs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d5532",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in test.trials:\n",
    "    ecf_ks_stat_t = {n: -1 for n in test.model.results_names}\n",
    "    for ks in test.ecf_ks_stat[t]:\n",
    "        for n, ks_stat in ks.items():\n",
    "            ecf_ks_stat_t[n] = max(ecf_ks_stat_t[n], ks_stat)\n",
    "    print(f'{t} trials')\n",
    "    for n, ks_stat in ecf_ks_stat_t.items():\n",
    "        print(f'\\t{n}: {ks_stat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db587dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_time = test.max_ks_stat_time(test.trials[-1])\n",
    "# preview_time = test.min_final_eval_time(test.trials[-1])\n",
    "\n",
    "test.plot_ecf(time=preview_time)\n",
    "test.plot_ecf_diffs()\n",
    "_ = test.plot_ecf_comparison(time=preview_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3085c8c",
   "metadata": {},
   "source": [
    "It also seems that we can provide predictions about how many samples are required to reach a certain convergence metric as samples are collected. When performing the so-called \"modeler workflow\", the Kolmogrov-Smirnov statistic seems to fit well to a power law as a function of the total number of replicates in the two sets of replicates, \n",
    "\n",
    "$$\n",
    "D \\left( N \\right) = a N ^ b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522852f0",
   "metadata": {},
   "source": [
    "This simulates the process of iteratively increasing the number of replicates and producing new predictions of the Kolmogrov-Smirnov statistic at each iteration: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e461ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.generate_ecf_diff_fits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d671df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = test.plot_ecf_diff_fits(test.plot_ecf_diffs())\n",
    "_ = axs[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c5d9f0",
   "metadata": {},
   "source": [
    "Matthias pointed out that the K-S statistic might be sensitive to how a sample is divided when testing for similarity. \n",
    "\n",
    "We find that Matthias was correct. The test for reproducibility should perform the same test repeatedly until a mean K-S statistic converges, which should be the reported final statistic for reproducibility. When the mean converges and the mean plus some amount of the standard deviation is below a threshold, consider the sample size sufficient for reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35cdcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: ~17 minutes (M1 max)\n",
    "test.test_sampling(err_thresh=1E-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e2a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test.plot_ks_sampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5950a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.generate_ecf_sampling_fits()\n",
    "_, ax = test.plot_ecf_sampling_fits(test.plot_ecf_sampling())\n",
    "_ = ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
